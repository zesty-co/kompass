{{- $globalValues := .Values.global | default (dict) -}}
{{- $validatorValues := .Values.validator | default (dict) -}}
{{- $validatorPodAnnotations := include "kompass.workload.resolveMap" (dict "global" $globalValues "component" $validatorValues "key" "podAnnotations") | fromYaml -}}
{{- $validatorPodLabels := include "kompass.workload.resolveMap" (dict "global" $globalValues "component" $validatorValues "key" "podLabels") | fromYaml -}}
{{- $validatorAffinity := include "kompass.workload.resolveMap" (dict "global" $globalValues "component" $validatorValues "key" "affinity") | fromYaml -}}
{{- $validatorImagePullSecrets := include "kompass.workload.resolveList" (dict "global" $globalValues "component" $validatorValues "key" "imagePullSecrets") | fromYamlArray -}}
{{- $validatorPodSecurityContext := include "kompass.workload.resolveMap" (dict "global" $globalValues "component" $validatorValues "key" "podSecurityContext") | fromYaml -}}
{{- $validatorSecurityContext := include "kompass.workload.resolveContainerSecurityContext" (dict "global" $globalValues "component" $validatorValues) | fromYaml -}}
{{- $validatorTopologySpreadConstraints := include "kompass.workload.resolveList" (dict "global" $globalValues "component" $validatorValues "key" "topologySpreadConstraints") | fromYamlArray -}}
{{- $validatorAutomountServiceAccountToken := include "kompass.workload.resolveBool" (dict "global" $globalValues "component" $validatorValues "key" "automountServiceAccountToken") | trim -}}
{{- $validatorRuntimeClassName := include "kompass.workload.resolveString" (dict "global" $globalValues "component" $validatorValues "key" "runtimeClassName") | trim -}}
{{- $validatorServiceAccountName := index $validatorValues "serviceAccountName" | default "" | trim | default "kompass-validator" -}}
---
# Service Account for the validation job
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ $validatorServiceAccountName }}
  namespace: {{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-10"  # Run before the validation job
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed,before-hook-creation
---
# RBAC Role with permissions to check for CSI driver and storage classes
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ $validatorServiceAccountName }}
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-10"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed,before-hook-creation
rules:
- apiGroups: ["storage.k8s.io"]
  resources: ["csidrivers", "storageclasses"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments", "daemonsets"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["namespaces", "services"]
  verbs: ["get", "list"]
---
# Bind the role to the service account
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: {{ $validatorServiceAccountName }}
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-10"
    "helm.sh/hook-delete-policy": hook-succeeded,hook-failed,before-hook-creation
subjects:
- kind: ServiceAccount
  name: {{ $validatorServiceAccountName }}
  namespace: {{ .Release.Namespace }}
roleRef:
  kind: ClusterRole
  name: {{ $validatorServiceAccountName }}
  apiGroup: rbac.authorization.k8s.io
---
# The validation job
apiVersion: batch/v1
kind: Job
metadata:
  name: kompass-validation
  namespace: {{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-5"  # Run after the service account is created
    "helm.sh/hook-delete-policy": hook-succeeded,before-hook-creation
spec:
  ttlSecondsAfterFinished: 300  # Keep the job for 5 minutes after failure for debugging
  template:
    metadata:
      name: kompass-validation
      {{- if $validatorPodAnnotations }}
      annotations:
{{ toYaml $validatorPodAnnotations | nindent 8 }}
      {{- end }}
      {{- $validatorDefaultPodLabels := dict "app.kubernetes.io/name" "kompass-validation" "app.kubernetes.io/instance" .Release.Name }}
      {{- $validatorResolvedPodLabels := mergeOverwrite (deepCopy $validatorDefaultPodLabels) $validatorPodLabels }}
      labels:
{{ toYaml $validatorResolvedPodLabels | nindent 8 }}
    spec:
      serviceAccountName: {{ $validatorServiceAccountName }}
      {{- if ne $validatorAutomountServiceAccountToken "null" }}
      automountServiceAccountToken: {{ $validatorAutomountServiceAccountToken }}
      {{- end }}
      {{- if $validatorImagePullSecrets }}
      imagePullSecrets:
{{ toYaml $validatorImagePullSecrets | nindent 8 }}
      {{- end }}
      {{- if $validatorPodSecurityContext }}
      securityContext:
{{ toYaml $validatorPodSecurityContext | nindent 8 }}
      {{- end }}
      {{- if $validatorRuntimeClassName }}
      runtimeClassName: {{ $validatorRuntimeClassName }}
      {{- end }}
      {{- if $validatorAffinity }}
      affinity:
{{ toYaml $validatorAffinity | nindent 8 }}
      {{- end }}
      {{- if $validatorTopologySpreadConstraints }}
      topologySpreadConstraints:
{{ toYaml $validatorTopologySpreadConstraints | nindent 8 }}
      {{- end }}
      {{- if .Values.global.nodeSelector }}
      nodeSelector:
      {{- toYaml .Values.global.nodeSelector | nindent 8 }}
      {{- end }}
      {{- if .Values.global.tolerations }}
      tolerations:
      {{- toYaml .Values.global.tolerations | nindent 8 }}
      {{- end }}
      containers:
      - name: {{ $validatorServiceAccountName }}
        image: {{ .Values.validator.image.registry }}/{{ .Values.validator.image.repository }}:{{ .Values.validator.image.tag }}
        {{- if $validatorSecurityContext }}
        securityContext:
{{ toYaml $validatorSecurityContext | nindent 10 }}
        {{- end }}
        command:
        - /bin/bash
        - -c
        - |
          # Check if global.storageClassName is set (not null and not empty string)
          if [ "{{ .Values.global.storageClassName }}" != "~" ] && [ "{{ .Values.global.storageClassName }}" != "" ]; then
            ##############################################################
            # Validate StorageClass and AWS EBS CSI Driver installation  #
            ##############################################################
            echo "Validating StorageClass and AWS EBS CSI Driver installation..."
            
            # Check for expected storage class
            STORAGE_CLASS_NAME="{{ .Values.victoriaMetrics.server.persistentVolume.storageClassName | default "ebs-sc" }}"
            if kubectl get storageclass $STORAGE_CLASS_NAME &> /dev/null; then
              echo "âœ… StorageClass '$STORAGE_CLASS_NAME' exists"
              
              # Verify that the storage class uses the AWS EBS CSI provisioner
              PROVISIONER=$(kubectl get storageclass $STORAGE_CLASS_NAME -o jsonpath='{.provisioner}')
              if [ "$PROVISIONER" = "ebs.csi.aws.com" ]; then
                echo "âœ… StorageClass uses AWS EBS CSI provisioner"
                
                # Check if the CSI driver is installed by looking for the CSIDriver object
                if kubectl get csidriver ebs.csi.aws.com &> /dev/null; then
                  echo "âœ… AWS EBS CSI Driver is installed"
                else
                  echo "âŒ AWS EBS CSI Driver is not installed"
                  echo "Please install the AWS EBS CSI Driver before proceeding."
                  echo "You can install it using the AWS EBS CSI Driver Helm chart or as an EKS add-on."
                  exit 1
                fi
                
                # Check for the controller deployment
                if kubectl get deployment -n kube-system ebs-csi-controller &> /dev/null; then
                  echo "âœ… AWS EBS CSI Controller deployment found"
                else
                  echo "âŒ AWS EBS CSI Controller deployment not found"
                  echo "The AWS EBS CSI Driver may not be properly installed."
                  exit 1
                fi
                
                # Check for the daemonset
                if kubectl get daemonset -n kube-system ebs-csi-node &> /dev/null; then
                  echo "âœ… AWS EBS CSI Node DaemonSet found"
                else
                  echo "âŒ AWS EBS CSI Node DaemonSet not found"
                  echo "The AWS EBS CSI Driver may not be properly installed."
                  exit 1
                fi

              else
                echo "âš ï¸ StorageClass '$STORAGE_CLASS_NAME' uses provisioner '$PROVISIONER', instead of 'ebs.csi.aws.com'"
                echo "Going to use existing non-EBS storage class"
                # Continue without validation of the AWS EBS CSI driver components
              fi
            else
              echo "âŒ StorageClass '$STORAGE_CLASS_NAME' does not exist"
              echo "Please create the required StorageClass with provisioner 'ebs.csi.aws.com'."
              exit 1
            fi
          else
            echo "â© Skipping AWS EBS CSI Driver validation - global.storageClassName is not set"

            # Check for default storage class when global.storageClassName is not set
            echo "Checking for default storage class in the cluster..."
            DEFAULT_STORAGE_CLASS=$(kubectl get storageclass -o=jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}')
            
            if [ -n "$DEFAULT_STORAGE_CLASS" ]; then
              echo "âœ… Found default StorageClass: $DEFAULT_STORAGE_CLASS"
            else
              echo "âŒ No default StorageClass found in the cluster"
              echo "âš ï¸ Without a default StorageClass or specified global.storageClassName, PVC creation may fail"
              echo "Please create a default StorageClass or specify global.storageClassName in the values file"
              exit 1
            fi
          fi

          # Print empty line for better readability
          echo ""

          #############################################
          # Validate kube-state-metrics installation  #
          #############################################

          KSM_ENABLED="{{ index .Values "kubeStateMetrics" "enabled" }}"

          if [ "$KSM_ENABLED" = "true" ]; then
            echo "Validating kube-state-metrics installation..."
            KSM_FORCE="{{ index .Values "kubeStateMetrics" "force" }}"
      
            if [ "$KSM_FORCE" = "true" ]; then
              echo "âš ï¸ kubeStateMetrics.force is set to true â€” kubeStateMetrics will be installed even if it already exists"
              echo "âœ… Skipping kubeStateMetrics validation"
            else
              # Find the namespace where kubeStateMetrics is installed
              KSM_NAMESPACE=$(kubectl get deployment --all-namespaces -l app.kubernetes.io/name=kube-state-metrics -o jsonpath='{.items[0].metadata.namespace}')
              if [ -z "$KSM_NAMESPACE" ]; then
                echo "âœ… kube-state-metrics deployment not found in any namespace - it will be installed as part of Zesty Kompass Pod-RightSizing"
              else
                echo "âœ… kube-state-metrics deployment found in namespace: $KSM_NAMESPACE"
                KSM_EXISTS=true
              fi
              
              if [ -n "$KSM_EXISTS" ] && [ "$KSM_NAMESPACE" != "{{ .Release.Namespace }}" ]; then
                # Retrieve the kube-state-metrics service details
                KSM_SVC=$(kubectl get svc -n $KSM_NAMESPACE -l app.kubernetes.io/name=kube-state-metrics -o jsonpath='{.items[0].metadata.name}')
                if [ -n "$KSM_SVC" ]; then
                    echo "âœ… kubeStateMetrics service found: $KSM_SVC in namespace: $KSM_NAMESPACE"
                    echo ""
                    echo "âš ï¸ Please add to the values file kubeStateMetrics.enabled: false AND kubeStateMetrics.serviceName: $KSM_SVC AND kubeStateMetrics.serviceNamespaec: $KSM_KSM_NAMESPACE OR use kubeStateMetrics.force: true"
                    exit 1
                else
                    echo "âš ï¸ kubeStateMetrics service not found in namespace: $KSM_NAMESPACE"
                    exit 1
                fi
              fi
            fi
          else
            KSM_SVC_NAME="{{ index .Values "kubeStateMetrics" "serviceName" | default "" }}"
            # Only check service name, not namespace, for backward compatibility
            if [ -z "$KSM_SVC_NAME" ]; then
              echo "âŒ kubeStateMetrics.enabled is set to false but kubeStateMetrics.serviceName is not provided"
              echo "âš ï¸ Please specify kubeStateMetrics.serviceName and kubeStateMetrics.serviceNamespace in your values file or set kubeStateMetrics.enabled to true"
              exit 1
            fi
            echo "âš ï¸ kubeStateMetrics.enabled is set to false â€” kubeStateMetrics will not be installed, going to use the existing service: $KSM_SVC_NAME"
            echo "âœ… Skipping kubeStateMetrics validation"
          fi
          echo ""

          #########################################
          # Validate cert-manager installation  #
          #########################################
          
          echo "Validating cert-manager installation..."
          CERT_MANAGER_ENABLED="{{ index .Values "cert-manager" "enabled" | default "false" }}"

          # Check if cert-manager exists in the cluster using labels
          # First try with the k8s-app label which is standard in most distributions
          if kubectl get deployment --all-namespaces -l app=cert-manager --no-headers 2>/dev/null | grep -q ""; then
            echo "âœ… cert-manager is installed in the cluster (found via app=cert-manager label)"
          # Then try with the app.kubernetes.io/name label which is recommended
          elif kubectl get deployment --all-namespaces -l app.kubernetes.io/name=cert-manager --no-headers 2>/dev/null | grep -q ""; then
            echo "âœ… cert-manager is installed in the cluster (found via app.kubernetes.io/name label)"
          # Finally fall back to checking for the standard deployment name
          elif kubectl get deployment cert-manager -n kube-system &> /dev/null; then
            echo "âœ… cert-manager is installed in the cluster (found via standard name)"
          else
            # If cert-manager is not found and not enabled in values
            if [ "$CERT_MANAGER_ENABLED" != "true" ]; then
              echo "âŒ cert-manager is not installed in the cluster"
              echo "âš ï¸ Please set cert-manager.enabled=true in your values file to install cert-manager"
              exit 1
            else
              echo "âš ï¸ cert-manager is not installed, it will be deployed as part of Zesty Kompass installation"
            fi
          fi
          echo ""

          echo "ğŸ‰ All validations passed! Your cluster is ready for Zesty Kompass installation."
      restartPolicy: Never
  backoffLimit: 0
